# 빅오

## 빅오

빅오는 입력값이 커질때 알고리즘의 실행 시간과 함꼐 공간 요구사항이 어떻게 증가하는지를 분류하는데 사용되며, 알고리즘의 효율성을 분석하는데 유용하게 활용된다. <br>
예를 들어, 디스크에 저장된 파일을 다른 지역에 살고 있는 친구에게 보낸다고 가정해보자. <br>
파일 크기가 작다면(n이 작다면) O(n)의 시간이 걸리는 온라인 전송이 빠르다. 하지만 파일이 아주 크다면 비행기를 통해 물리적으로 배달하는게 더 빠를 수 있다 O(1).

<img width="446" alt="Image" src="https://github.com/user-attachments/assets/613692b1-6303-4c0c-a43d-a9f14c8d69af" />

점근적 실행 시간을 달리 말하면 시간복잡도라 할 수 있다. <br>
시간복잡도의 사전적 정의는 어떤 알고리즘을 수행하는데 걸리는 시간을 설명하는 계산 복잡도이며, 이를 표기하는 대표적인 방법이 빅오다. <br>
빅오로 시간복잡도를 표현할때는 최고차 항만을 표기하며, 계수는 무시한다.

- O(1): 입력값이 아무리 커도 실행 시간이 일정하다.
- O(log n): 로그는 매우 큰 입력값에도 크게 영향을 받지 않는 함수이다. 대표적으로 이진 검색이 이에 해당한다.
- O(n): 입력값의 크기만큼 실행 시간에 영향을 받으며, 실행 시간이 선형으로 증가한다.
- O(nlog n): 입력값만큼 순회하며 log n 의 연산이 곱해진다. 병합 정렬을 비롯한 효율적인 정렬 알고리즘이 이에 해당한다.
- O(n^2): 입력값의 제곱만큼 연산한다. 버블 정렬같은 비효율적인 정렬 알고리즘이 이에 해당한다.
- O(2^n): 입력값의 크기만큼 2배씩 연산한다. 피보나치 수를 재귀로 계산하는 알고리즘이 이에 해당한다.
- O(n!): 입력값을 1씩 줄여가며 곱셈 연산을 한다.


### 빅오를 계산하는 실용적인 방법

다음과 같이 함수를 리턴으로 종료하기 직전에 연산 횟수를 헤아려보자. <br>
몇 번이나 연산했는지 count 변수로 일일이 연산 횟수를 헤아려보면 정확하게 알 수 있다. <br>
출력 결과를 보면 시간복잡도는 O(n+1)이고 상수는 생략하는 O(n)이다.

```java
int count = 0;
public int factorial(int n) {
    if (n >= 1) {
        count++;
        return n * factorial(n - 1);
    } else {
        count++;
        return 1;
    }
}
```

```
factorial(5), count = 6
factorial(6), count = 7
factorial(7), count = 8
```

### 상한과 최악

빅오는 상한을 의미한다. 이외에도 하한을 나타내는 빅오메가, 평균을 의미하는 빅세타가 있다. <br>
빅오 표기법은 정확하게 쓰기에는 너무 길고 복잡한 함수를 적당히 정확하게 표현하는 방법일 뿐, 최악/평균적인 경우의 시간복잡도와는 아무 관계가 없다. <br>
예를 들어, 퀵 정렬의 시간 복잡도는 다음과 같이 정리할 수 있다.

`퀵 정렬의 시간복잡도는 최악의 경우 O(n^2)이고, 최선의 경우 O(nlog n)이다.`

### 분할 상환 분석

분할 상환 분석은 알고리즘의 복잡도를 계산할때 알고리즘 전체를 보지 않고 최악의 경우만을 살펴보는 것은 지나치게 비관적이라는 이유로 등장했는데, 대표적인 예로 동적 배열을 들 수 있다. <br>
동적 배열은 엘리먼트를 하나씩 추가하다가 용량이 일정 비율 이상 차면 용량을 확장하고 엘리먼트를 모두 복사하는 더블링이라는 작업을 하는데, 이는 어쩌다 한 번 일어나는 일이지만 이로인해 동적 배열의 시간복잡도는 O(n)이라고 얘기하는 것은 지나치게 비관적이다. <br>
따라서 이런 경우 최악의 경우인 O(n)을 여러번에 걸쳐 골고루 나눠주는 형태로 알고리즘의 시간복잡도를 계산할 수 있으며, 이를 분할 상환이라고 표현한다. <br>
이렇게 할 경우 동적 배열의 삽입시 시간 복잡도는 O(1)이 된다.

### 복잡도의 특징

빅오는 시간복잡도 외에도 공간복잡도를 표현하는데도 쓰인다. <br>
알고리즘은 시간과 공간의 트레이드오프 관계를 이룬다고 하는데, 실행 시간이 빠른 알고리즘은 공간을 많이 사용하고, 공간을 적게 차지하는 알고리즘은 실행 시간이 느리다는 얘기다.

## 자바 컬렉션 프레임워크의 빅오

### 리스트의 시간 복잡도

<img width="534" alt="Image" src="https://github.com/user-attachments/assets/12f61e7e-a0d1-4b5e-baee-434a8ffd639d" />

ArrayList는 동적 배열이고, LinkedList는 연결 리스트로 구현되어 있다. <br>

ArrayList의 인덱스 끝에 엘리먼트를 삽입하는 것은 O(1)로 가능하다. 그러나 공간이 가득찰 경우 더블링이 일어나고 O(n)이 소요된다. <br>
인덱스 중간에 삽입하는 시간은 O(n)이다. 연속된 배열 구조에서는 중간에 삽입할 수 있는 방법이 없기 때문에, 신규 엘리먼트를 포함하여 전체를 새로운 공간에 복사애햐 한다. <br>
인덱스 끝에서 삭제는 O(1)로 가능하지만 중간에서 삭제하려면 O(n)이다. <br>
배열의 가장 큰 장점은 어느 위치에 있든 인덱스를 지정하면 O(1)로 조회가 가능하다는 점이다. 항상 지정한 순서대로 연속된 배열에 위치하므로 즉시 조회가 가능하다.

LinkedList는 인덱스 끝에 삽입이 O(1)로 가능하지만, 인덱스 중간에 삽입하려면 해당 위치까지 거슬러 내려가야 한다. 그래서 탐색에 O(n)이 필요하다. <br>
하지만 삽입 자체는 노드를 연결해주기만 하면 되서 O(1)이다. 대개 이 과정을 합쳐서 연결리스트에서 중간에 삽입하는 연산을 O(n)이라고 한다. <br>
삭제도 마찬가지로 끝에 삭제는 O(1)이지만 인덱스 중간에 삭제하려면 해당 위치까지 거슬러 내려가는데 O(n)이 필요하다. <br>
연결 리스트는 조회가 문제다. 바로 해당 위치를 찾을 수 있는 배열과 달리 연결리스트는 매번 탐색해야 하므로 조회시 O(n)이다. 조회가 잦다면 연결리스트 구현인 LinkedList는 지양해야 한다.

### 맵 시간 복잡도

<img width="343" alt="Image" src="https://github.com/user-attachments/assets/81b0bf7e-ad19-49c7-968f-ef6c1c613d23" />

HashMap은 추가, 삭제, 조회 모두 O(1)에 가능하다. 물론 최악의 경우에는 키 충돌이 발생해 O(n)이 될 수 있다. <br>
LinkedHashMap은 입력 순서를 보장한다. 원래 해시 테이블은 입력 순서가 보장되지 않지만, 연결 리스트를 함꼐 사용해 입력 순서를 유지한다.

### 데크 시간 복잡도

<img width="314" alt="Image" src="https://github.com/user-attachments/assets/92cdc050-c734-4617-99b5-73060b928095" />

데크는 양쪽에서 삽입과 삭제를 할 수 있는, 스택과 큐의 연산을 모두 갖고 있는 자료형이다. <br>
원래 자료형의 성격은 이중 연결 리스트가 어울려서 LinkedList가 맞지만, 실제로는 동적 배열로 구현된 ArrayDeque가 좀 더 널리 사용된다. <br>
자바의 Stack 자료형은 성능에 문제가 있어, 스택을 구현할때 ArrayDeque를 사용하는것이 좋다.















